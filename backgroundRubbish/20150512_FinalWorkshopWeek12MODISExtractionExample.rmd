---
title: "Demonstration of using satellite data in rainfall runoff modelling"
author: "Willem Vervoort"
date: "Monday May 11, 2015"
output: word_document
---

This workshop will demonstrate how you can use satellite MODIS16 ET data in a rainfall runoff model for the Karuah catchments. It will go through the download of satellite data, the conversion of satellite data and the extraction of satellite data for the location, followed by the inclusion of the data into the model and how you would use this in calibration. This workshop we will concentrate on the Crabapple catchment at Karuah again, but for the assessment task, you will have to repeat part of the script for another catchment at Karuah. 

# Packages
For this we will use the MODISTools package in R, this can be installed in R using the command `install.packages("MODISTools")`. Instructions can also be found in the package manual. We will also use several other packages whcih assist with mapping and reprojecting. We will also use the package hydromad to do the modelling as this will allow using several conceptual rainfall runoff models. For the assessment task and for the workshop, we have already downloaded the satellite data, as this takes a long time.

###Step 1. 
We will first load the packages and load the rainfall, flow and temperature data from the Karuah catchments. This is the original data and follows the format of the zoo data frame as described in the tutorial of the package hydromad. The flow data and rainfall are in mm, the E (potential evaporation) data are the daily maximum temperatures, which can be used as a proxy for potential E. This all just follows the workshop in week 2 and 3.

For your assessment task you need to use another catchment, how you make the zoo data file for Hydromad for this is at the end of the script for week 4 ("extension") or in your assessment task you did in week 4.

```{r preliminaries}
require(MODISTools)
require(hydromad)
# set proxy on Sydney campus
Sys.setenv(http_proxy="web-cache.usyd.edu.au:8080")

# load older flow and climate data for Karuah, see workshop 2
load("z:/willem/teaching/lwsci4/karuah/20150319_HydromadCalibration.RData")

```
### Step 2
Now define the Latitudes and Longitudes necessary to extract the points and download the MODIS data
This step takes quite some time, as this involves connecting to the server and downloading the data using the package MODISTools. The `coords` data.frame should list the coordinates for all the pixels that we are going to extract on the transect. The argument `Size` indicates the number of pixels around the central pixel that needs to be downloaded. In this case we have coordinates for all the pixels and we just download the pixels using the function `MODISSubsets`. I have commented this statement out in this document as this takes very long.


```{r KaruahDataPoints}
setwd("z:/willem/teaching/lwsci4/week 3/tutorial")
## Download and work with the MODIS reprojection tool to reproject tiles

## read in the spatial points (locations of weirs)
Stations <- read.csv("Station_locations.csv")
# reading in the MODIS file
# use modis tools to extract one pixel for each catchment as the catchments are very small
coords = data.frame(lat = Stations[,2], long = Stations[,3])
# add start and end date and convert from julian to dates
start = substr(GetDates(Lat = Stations[1,2], Long = Stations[1,3], Product = "MOD16A2")[1],2,8)
coords$start.date=as.POSIXct(start,format="%Y%j")
end = substr(GetDates(Lat = Stations[1,2], Long = Stations[1,3], Product = "MOD16A2"),2,8)
coords$end.date=as.POSIXct(end[length(end)],format="%Y%j")
# Now extract all the timeseries for these modis pixels
# (Commented out as this takes very long)
# We need to figure out the name of the product, you can use GetProducts()
# GetProducts()
# # and check out the data bands that are in the product
# GetBands(Product="MOD16A2")
# 
# # # Now use MODISSubsets (Best to do this in blocks if it bombs out)
# for (i in 1:nrow(coords)) {
#   MODISSubsets(LoadDat=coords[i,], Product = "MOD16A2",
#                 Bands=c("ET_1km","ET_QC_1km"), StartDate=T, Size=c(0,0),
#                 SaveDir="z:/willem/teaching/lwsci4/MODIS")
# # be nice to the server, rest a minute
#   Sys.sleep(60)
# }

```


### Step 3
Once we have all the Satellite ET data extracted, we need to calculate some sort of average for the catchment and than we can merge this with the original data (Cotter) and fit a hydrological model.

```{r ETsummary, fig.width=6,fig.height=6}
x1 <- list.files (path="z:/willem/teaching/lwsci4/MODIS",pattern= (".asc"))
n <- nrow(read.csv(paste("z:/willem/teaching/lwsci4/MODIS/",x1[1],sep=""),header=F))

Store <- data.frame(Year = numeric(length=n),
                    Jdate = numeric(length=n),
                    ET = numeric(length=n))
Store1 <- list()

for (i in 1:length(x1)) {
  Mdata <- read.csv(paste("z:/willem/teaching/lwsci4/MODIS/",x1[i],sep=""),header=F)
  Store[,1] <- as.numeric(substr(Mdata[,8],2,5))
  Store[,2] <- as.numeric(substr(Mdata[,8],6,8))
  Store[,3] <- Mdata[,11]/10 ##  0.1 scaling factor (see MODIS read_me)
  Store1[[i]] <- Store
}


test <- do.call(rbind,Store1)
# check distribution
hist(test$ET, main="Histogram of raw ET values across all pixels", 
     xlab="8 daily sum ET (mm)")

ET.mean <- aggregate(test[,3],list(Jdate=test$Jdate, Year=test$Year), 
                     mean,na.rm=T)
# want to make a vector with the number of days between observations (MODIS accumulation days)
ET.mean$Year[nrow(ET.mean)] # last year is not a leap year
# this calculates the difference between the julian dates, 
# but this means the last day gets -360, for non-leap years should be 5
days.cor <- c(diff(ET.mean$Jdate),5)
# Now we need to replace the -360 values, 2000 is a leap year
# there are 14 years alltogether
leap <- c(rep(c(6,rep(5,3)),3),5,5)
days.cor[days.cor==-360] <- leap

# for testing
#length(days.cor)
#length(ET.mean$x)


## get dates for each 8-day tile
for (i in 1:length(days.cor)) {
  if (i ==1) {
   dates.tiles <- seq.Date(as.Date("2000-01-01"),
                            by=days.cor[i],length.out=2)
   } else {
     dates.tiles <- c(dates.tiles,
                      (seq.Date(as.Date(dates.tiles[length(dates.tiles)]),
                       by=days.cor[i],length.out=2))[2])
   }
}
# cut-off last day, need to check code above and rewrite to make sure it does not add the extra day.
dates.tiles <- dates.tiles[-690]

#plot(as.Date(dates.tiles),ET.mean$x)

```

## Step 4 Hydromad fitting the original model
The following section shows how to use this data in  fitting routine with the package Hydromad. In this case we will fit the model GR4J (Perrin et al. 2003 J Hydro 279:275-€“289). We can now create the calibration and validation data.
```{r cal_val}
data.cal <- window(Karuah, start = "2000-01-01",end = "2006-12-31")

```
### fitting the model
The next step is to define a GR4J model using the HYDROMAD package. We did this already in week 3 and is also spelled out on the <a href="http://hydromad.catchment.org">HYDROMAD website</a>. 
```{r Model}
#hydromad.options(trace=TRUE)
#options(warn=1)

# Define the model, important to define return_state=T
# parameter guesses based on week 3
Crab.Q <- hydromad(DATA=data.cal,
          sma = "gr4j", routing = "gr4jrouting", 
          x1 = c(100,1500), x2 = c(-30,20), x3 = c(5,500), x4 = c(0.5,10), 
          etmult=c(0.01,0.5), 
          return_state=TRUE)
# use Viney's objective function(includes Bias), to fit 
# see http://hydromad.catchment.org/#hydromad.stats
hydromad.stats("viney" = function(Q, X, ...) {
    hmadstat("r.squared")(Q, X, ...) -
      5*(abs(log(1+hmadstat("rel.bias")(Q,X)))^2.5)})

# Using shuffled complex evolution algorithm for fitting
Crabfit.Q<- fitBySCE(Crab.Q,  
        objective=~hmadstat("viney")(Q,X))
		 
summary(Crabfit.Q)
coef(Crabfit.Q)
hmadstat("viney")(Q=data.cal$Q,X=Crabfit.Q$fitted.values)
```
It comes up with a reasonble fit with a an NSE of `r round(as.numeric(summary(Crabfit.Q)[8]),2)`. The "loss" term in the routing component (`x2`) positive: `r round(as.numeric(coef(Crabfit.Q)[1]),2)`, and `x3` the capacity of the groundwater store is medium `r round(as.numeric(coef(Crabfit.Q)[2]),2)`. 
These results are for the traditional fit on stream flow, and do not use the actual ET values from the MODIS satellite. To do this we need to set up the calibration to simultaneously calibrate on both the MODIS aET and the streamflow. Here the weighting factor w is set to 0.5 following Vervoort et al. (2014).

## Step 5 adding the MODIS data and refitting the model
We will now add the MODIS ET as a column to the dataset. This is not that easy, as Hydromad throws out all the missing values. So if we just add the ET data for every 8 days, the calibration will be only on every 8th day. There is a bit of trickery that we have to do to make this all work.

```{r merge_data}
# Make Modis ET data set into a zoo series
aET <- zoo(ET.mean$x, order.by=dates.tiles )

# create a new vector of dates for the tiles.
# However we want to expand to make sure each date is repeated 8 times to later aggregate the predicted ET and make it into a zoo data
# we use dates.tiles and days.cor (the number of days between tiles)
date.sum <- zoo(rep(dates.tiles, days.cor),
                order.by= seq.Date(as.Date(dates.tiles[1]),by=1,length=sum(days.cor)))

# now merge ETa with Karuah
Flow.zoo <- merge(data.cal ,aET=aET, et.period=date.sum,all=T)

# remake the calibration data
data.cal <- window(Flow.zoo, start = "2000-01-01",end = "2006-12-31")
# insert 0 values for the missing values to make hydromad think you have all data
# inserting 0 values also helps with defining the objective function as we will see later
data.cal[is.na(data.cal$aET),"aET"] <- 0
# show what it looks like
head(data.cal,12)
```
Now we can finally use this data into hydromad to fit the model.

```{r fitting_aET, fig.width=6,fig.height=6}
# This is one way to do this, but uses the NSE
# Define an objective function that aggregates ET based on specified periods
# use buildTsObjective, but this uses the NSE
hydromad.stats("ETfun" = function(...,DATA,U) {
    .(buildTsObjective(DATA$aET, groups=DATA$et.period, FUN=sum))(DATA$aET,U$ET,...)})

# redefine the model
Crab.Q <- hydromad(DATA=data.cal,
          sma = "gr4j", routing = "gr4jrouting", 
          x1 = c(100,1500), x2 = c(-30,20), x3 = c(5,500), x4 = c(0.5,10), 
          etmult=c(0.01,0.5), 
          return_state=TRUE)


# Evaluate the model using the objective
w=0.5
Crabfit.both_a <- fitBySCE(Crab.Q, 
                  objective=~w*hmadstat("viney")(X,Q) +
                    (1-w)*hmadstat("ETfun")(DATA=DATA,U=U))

summary(Crabfit.both_a)
coef(Crabfit.both_a)
hmadstat("viney")(Q=data.cal$Q,X=Crabfit.both_a$fitted.values)
hmadstat("ETfun")(DATA=data.cal,U=Crabfit.both_a$U)
# the Q calibration
xyplot(Crabfit.both_a)

# the ET calibration
plot(data.cal$aET[data.cal$aET>0,], xlab="Date", ylab="Actual ET (mm/day)", col="red",
     lwd=4, lty=2,ylim=c(0,max(data.cal$aET)+1), main = "ETfun using NSE")
lines(zoo(aggregate(Crabfit.both_a$U$ET,
                list(date=data.cal$et.period),sum),order.by=dates.tiles))
legend("topleft",c("MODIS ET", "Predicted aET"),lwd=c(3,1),col=c("red",1),lty=c(2,1))


# This might be a better way, uses "viney"
# # Use buildTsObjective to create a new objective function
# Define an objective function that aggregates ET based on specified periods
hydromad.stats("ETaggrViney" = function(..., DATA,U) {
  #This should be mean if the observed aET is repeated for each point in the period
  # using sum because i inserted 0 values
  # inserted "coredata" statement after discussion with J Guillaume
  # relates to how bias is calculated
  aET.fin <- aggregate(DATA$aET,list(date=coredata(DATA$et.period)),sum)
  ET.fin <- aggregate(U$ET,list(date=coredata(DATA$et.period)),sum)
  #This can be any objective function
  obj <- hmadstat("viney")(coredata(aET.fin),coredata(ET.fin))
  return(obj)
})


# Now fit the model again
w = 0.5
Crabfit.both <- fitBySCE(Crab.Q, 
                  objective=~w*hmadstat("viney")(X,Q) +
                    (1-w)*hmadstat("ETaggrViney")(DATA=DATA,U=U))


summary(Crabfit.both)
coef(Crabfit.both)
hmadstat("viney")(Q=data.cal$Q,X=Crabfit.both$fitted.values)
hmadstat("ETaggrViney")(DATA=data.cal,U=Crabfit.both$U)
# the Q calibration
xyplot(Crabfit.both)

# the ET calibration
plot(data.cal$aET[data.cal$aET>0,], xlab="Date", ylab="Actual ET (mm/day)", col="red",
     lwd=4, lty=2,ylim=c(0,max(data.cal$aET)+1), main = "ETfun using Viney")
lines(zoo(aggregate(Crabfit.both$U$ET,
                list(date=data.cal$et.period),sum),order.by=dates.tiles))
legend("topleft",c("MODIS ET", "Predicted aET"),lwd=c(3,1),col=c("red",1),lty=c(2,1))

#save.image("20150511_Sessiondata.Rdata")

```
As we can see, this leads to a fit that is not as good as the original model. comparing the NSE of the original model `r round(as.numeric(summary(Crabfit.Q)[8]),2)` with the NSE of the model calibrated on both ET and Q: `r round(as.numeric(summary(Crabfit.both)[8]),2)`.
This is partly because the model is forced to fit both the ET data and the streamflow data. As the last figure shows, the fit on the ET is not very good. The predicted aET is only about half the MODIS ET.
It also leads to changes in the parameters. The "loss" term in the routing component (`x2`) increases: `r round(as.numeric(coef(Crabfit.Q)[1]),2)` versus `r round(as.numeric(coef(Crabfit.both)[1]),2)`, strongly decreased `x3` from `r round(as.numeric(coef(Crabfit.Q)[2]),2)` to `r round(as.numeric(coef(Crabfit.both)[2]),2)`, and there are similar changes in `x4` and `x1`. 

## Step 6 Validation of both models
```{r validation}
data.val <- window(Flow.zoo, start = "2005-01-01",end = "2006-12-31")
# insert 0 values for the missing values to make hydromad think you have all data
# inserting 0 values also helps with defining the objective function as we will see later
data.val[is.na(data.val$aET),"aET"] <- 0
head(data.val)

# validation of Q only
Crabfit.Q.val <- update(Crabfit.Q, newdata=data.val)
summary(Crabfit.Q.val)
# validation of Q and ET
Crabfit.both.val <- update(Crabfit.both, newdata=data.val)
summary(Crabfit.both.val)
```
In both cases the NSE for the validation is much lower than for the calibration. This could be due to changes in the structure of the data, possibly due to the worsening drought and the major rainfall in 2007.


